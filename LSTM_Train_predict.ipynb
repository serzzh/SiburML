{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import datetime\n",
    "from utils_laj import *\n",
    "from data_processing import get_SensorData, my_pca, series_to_supervised, lstm_sampling\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "today = datetime.date.today()\n",
    "TR_END = \"2017-12-31 23:00:00\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_checkpoint = './save/save_lstm/lstm_2_layers'\n",
    "input_path = './input_data'\n",
    "file = os.path.join(input_path,'sensors.csv')\n",
    "target_file = os.path.join(input_path,'coke_target.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature shape: (13272, 48)\n",
      "target length: (10400, 2)\n",
      "Explained variance [0.51403503 0.08175391 0.05176499 0.04896774 0.03780492 0.03676345\n",
      " 0.02541251 0.024164   0.0203316  0.01454788 0.01258437 0.00978744\n",
      " 0.00954984 0.00903366 0.00768036 0.00587745 0.00560218 0.00510874\n",
      " 0.00442867 0.0041226 ], Cumsum [0.51403503 0.59578894 0.64755393 0.69652167 0.73432659 0.77109004\n",
      " 0.79650255 0.82066655 0.84099815 0.85554603 0.86813039 0.87791783\n",
      " 0.88746767 0.89650134 0.90418169 0.91005914 0.91566132 0.92077006\n",
      " 0.92519873 0.92932133]\n"
     ]
    }
   ],
   "source": [
    "# Считываем данные из файлов, нормализуем и применяем метод главных комполнентов\n",
    "X, y, submit_X, mean_y, std_y = get_SensorData(file, target_file, nc=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data X: (10400, 20), y (10400, 2), submit_X (13272, 20)\n"
     ]
    }
   ],
   "source": [
    "# Размерности датасета с известным целевым y и полного датасета\n",
    "print(\"Data X: %s, y %s, submit_X %s\" % (X.shape, y.shape, submit_X.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples length 7480\n",
      "Targets length 7480\n",
      "Samples length 2872\n",
      "Targets length 2872\n",
      "Samples length 13224\n",
      "Targets length 0\n",
      "Data x_train, y_train: ((7480, 48, 20),(7480, 48));  x_test, y_test: ((2872, 48, 20),(2872, 48)); Xs, ys: ((13224, 48, 20),None) \n"
     ]
    }
   ],
   "source": [
    "# выделим последние записи в размере контрольного датасета в тестовый датасет\n",
    "x1_train, x1_test, y1_train, y1_test = train_test_split(X, y, test_size=2920, random_state=42, shuffle=False)\n",
    "\n",
    "# Формируем сэмплы для LSTM по 48 рядов внахлест с шагом 1\n",
    "timesteps = 48\n",
    "lag = 1\n",
    "i_train = len(x1_train) + timesteps  \n",
    "\n",
    "\n",
    "x_train, y_train = lstm_sampling(X[:i_train], y=y[:i_train]['target'], timesteps=timesteps, lag=lag)\n",
    "x_test, y_test = lstm_sampling(x1_test, y=y1_test['target'], timesteps=timesteps, lag=lag)\n",
    "#yl = y['target'][timesteps:]\n",
    "\n",
    "# формируем данные сабмита по аналогичному принципу + берем 47 шагов из тестовых данных\n",
    "Xs, ys = lstm_sampling(submit_X, timesteps=timesteps, lag=lag)\n",
    "\n",
    "print(\"Data x_train, y_train: (%s,%s);  x_test, y_test: (%s,%s); Xs, ys: (%s,%s) \" % (x_train.shape, y_train.shape, x_test.shape, y_test.shape, Xs.shape, ys))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "    Train = True\n",
    "    Predict = True\n",
    "    plot = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "    batch_size = 359  # Batch size\n",
    "    shift = batch_size\n",
    "    #if Train == False: batch_size = 1\n",
    "\n",
    "    sequence_length = timesteps  # Number of steps\n",
    "    learning_rate = 8*10e-5  # 0.0001\n",
    "    epochs = 1000\n",
    "    ann_hidden = 16\n",
    "\n",
    "    n_channels = x_train.shape[2]\n",
    "\n",
    "    lstm_size = 48  # Number LSTM units\n",
    "    num_layers = 2  # 2  # Number of layers\n",
    "    alpha = 0 # regularization coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My Conv Shape: [None, 48, 20]\n"
     ]
    }
   ],
   "source": [
    "    X = tf.placeholder(tf.float32, [None, sequence_length, n_channels], name='inputs')\n",
    "    Y = tf.placeholder(tf.float32, [None, sequence_length], name='labels')\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    learning_rate_ = tf.placeholder(tf.float32, name='learning_rate')\n",
    "    is_train = tf.placeholder(dtype=tf.bool, shape=None, name=\"is_train\")\n",
    "\n",
    "    conv_last_layer = X\n",
    "\n",
    "    shape = conv_last_layer.get_shape().as_list()\n",
    "    print('My Conv Shape:',shape)\n",
    "    CNN_flat = tf.reshape(conv_last_layer, [-1, shape[1] * shape[2]])\n",
    "\n",
    "    dence_layer_1 = dense_layer(CNN_flat, size=sequence_length * n_channels, activation_fn=tf.nn.relu, batch_norm=False,\n",
    "                                phase=is_train, drop_out=True, keep_prob=keep_prob,\n",
    "                                scope=\"fc_1\")\n",
    "    lstm_input = tf.reshape(dence_layer_1, [-1, sequence_length, n_channels])\n",
    "\n",
    "    cell = get_RNNCell(['LSTM'] * num_layers, keep_prob=keep_prob, state_size=lstm_size)\n",
    "    init_states = cell.zero_state(batch_size, tf.float32)\n",
    "    \n",
    "    # For each layer, get the initial state. states will be a tuple of LSTMStateTuples.\n",
    "    states = get_state_variables(batch_size, cell)\n",
    "\n",
    "    # Unroll the LSTM\n",
    "    rnn_output, new_states = tf.nn.dynamic_rnn(cell, lstm_input, dtype=tf.float32, initial_state=states)\n",
    "    \n",
    "    # Add an operation to update the train states with the last state tensors.\n",
    "    update_op = get_state_update_op(states, new_states)\n",
    "    reset_op = get_state_update_op(states, init_states)\n",
    "    \n",
    "    stacked_rnn_output = tf.reshape(rnn_output, [-1, lstm_size])  # change the form into a tensor\n",
    "\n",
    "    dence_layer_2 = dense_layer(stacked_rnn_output, size=ann_hidden, activation_fn=tf.nn.relu, batch_norm=False,\n",
    "                                phase=is_train, drop_out=True, keep_prob=keep_prob,\n",
    "                                scope=\"fc_2\")\n",
    "    \n",
    "    dence_layer_3 = dense_layer(dence_layer_2, size=ann_hidden, activation_fn=tf.nn.relu, batch_norm=False,\n",
    "                                phase=is_train, drop_out=True, keep_prob=keep_prob,\n",
    "                                scope=\"fc_2_2\")\n",
    "\n",
    "    output = dense_layer(dence_layer_3, size=1, activation_fn=None, batch_norm=False, phase=is_train, drop_out=False,\n",
    "                         keep_prob=keep_prob,\n",
    "                         scope=\"fc_3_output\")\n",
    "\n",
    "    prediction = tf.reshape(output, [-1])\n",
    "    y_flat = tf.reshape(Y, [-1])\n",
    "\n",
    "    h = prediction - y_flat\n",
    "    \n",
    "    tv = tf.trainable_variables()\n",
    "    regularization_cost = tf.reduce_sum([ tf.nn.l2_loss(v) for v in tv ])\n",
    "\n",
    "    cost_function = tf.reduce_sum(tf.square(h)) + alpha*regularization_cost\n",
    "    RMSE = tf.sqrt(tf.reduce_mean(tf.square(h)))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate_).minimize(cost_function)\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    training_generator = batch_generator(x_train, y_train, batch_size, sequence_length, online=True, online_shift=shift)\n",
    "    testing_generator = batch_generator(x_test, y_test, batch_size, sequence_length, online=True, online_shift=shift)\n",
    "    #print(len(list(training_generator)))\n",
    "\n",
    "    if Train: model_summary(learning_rate=learning_rate, batch_size=batch_size, lstm_layers=num_layers,\n",
    "                            lstm_layer_size=lstm_size, fc_layer_size=ann_hidden, sequence_length=sequence_length,\n",
    "                            n_channels=n_channels, path_checkpoint=path_checkpoint, spacial_note='')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set MSE\n",
      "No epoches:  1000 No itr:  20\n",
      "LSTM epoch: 0 RMSE-train: 0.8827776 RMSE-test 0.73666626 lr 0.0008 \ttime/epoch: 4.16 \ttime_remaining:  1  hr: 9.4  min \ttime_stamp:  2018.11.16-10:33:02\n",
      "LSTM epoch: 1 RMSE-train: 0.8822107 RMSE-test 1.0493764 lr 0.0008 \ttime/epoch: 3.14 \ttime_remaining:  0  hr: 52.3  min \ttime_stamp:  2018.11.16-10:33:05\n",
      "LSTM epoch: 2 RMSE-train: 0.856976 RMSE-test 0.76409984 lr 0.0008 \ttime/epoch: 3.12 \ttime_remaining:  0  hr: 52.0  min \ttime_stamp:  2018.11.16-10:33:08\n",
      "LSTM epoch: 3 RMSE-train: 0.8702405 RMSE-test 1.0333407 lr 0.0008 \ttime/epoch: 3.08 \ttime_remaining:  0  hr: 51.2  min \ttime_stamp:  2018.11.16-10:33:11\n",
      "LSTM epoch: 4 RMSE-train: 0.84990287 RMSE-test 0.8229849 lr 0.0008 \ttime/epoch: 3.03 \ttime_remaining:  0  hr: 50.4  min \ttime_stamp:  2018.11.16-10:33:14\n",
      "LSTM epoch: 5 RMSE-train: 0.855796 RMSE-test 1.0158867 lr 0.0008 \ttime/epoch: 3.03 \ttime_remaining:  0  hr: 50.3  min \ttime_stamp:  2018.11.16-10:33:17\n",
      "LSTM epoch: 6 RMSE-train: 0.84434193 RMSE-test 0.7349585 lr 0.0008 \ttime/epoch: 3.14 \ttime_remaining:  0  hr: 52.0  min \ttime_stamp:  2018.11.16-10:33:21\n",
      "LSTM epoch: 7 RMSE-train: 0.8554848 RMSE-test 1.1367859 lr 0.0008 \ttime/epoch: 3.05 \ttime_remaining:  0  hr: 50.5  min \ttime_stamp:  2018.11.16-10:33:24\n",
      "LSTM epoch: 8 RMSE-train: 0.83971363 RMSE-test 0.69393194 lr 0.0008 \ttime/epoch: 3.08 \ttime_remaining:  0  hr: 51.0  min \ttime_stamp:  2018.11.16-10:33:27\n",
      "LSTM epoch: 9 RMSE-train: 0.8639125 RMSE-test 0.97250724 lr 0.0008 \ttime/epoch: 3.05 \ttime_remaining:  0  hr: 50.3  min \ttime_stamp:  2018.11.16-10:33:30\n",
      "LSTM epoch: 10 RMSE-train: 0.83809817 RMSE-test 0.7474185 lr 0.0008 \ttime/epoch: 3.13 \ttime_remaining:  0  hr: 51.6  min \ttime_stamp:  2018.11.16-10:33:33\n",
      "LSTM epoch: 11 RMSE-train: 0.8362807 RMSE-test 0.8786269 lr 0.0008 \ttime/epoch: 3.13 \ttime_remaining:  0  hr: 51.5  min \ttime_stamp:  2018.11.16-10:33:36\n",
      "LSTM epoch: 12 RMSE-train: 0.8076967 RMSE-test 0.65271664 lr 0.0008 \ttime/epoch: 3.19 \ttime_remaining:  0  hr: 52.5  min \ttime_stamp:  2018.11.16-10:33:39\n",
      "LSTM epoch: 13 RMSE-train: 0.8001474 RMSE-test 0.92157054 lr 0.0008 \ttime/epoch: 3.09 \ttime_remaining:  0  hr: 50.8  min \ttime_stamp:  2018.11.16-10:33:42\n",
      "LSTM epoch: 14 RMSE-train: 0.8378449 RMSE-test 0.63039064 lr 0.0008 \ttime/epoch: 3.11 \ttime_remaining:  0  hr: 51.1  min \ttime_stamp:  2018.11.16-10:33:45\n",
      "LSTM epoch: 15 RMSE-train: 0.8500133 RMSE-test 0.825934 lr 0.0008 \ttime/epoch: 3.16 \ttime_remaining:  0  hr: 51.9  min \ttime_stamp:  2018.11.16-10:33:49\n",
      "LSTM epoch: 16 RMSE-train: 0.79684526 RMSE-test 0.69805485 lr 0.0008 \ttime/epoch: 3.12 \ttime_remaining:  0  hr: 51.1  min \ttime_stamp:  2018.11.16-10:33:52\n",
      "LSTM epoch: 17 RMSE-train: 0.78792304 RMSE-test 0.78881395 lr 0.0008 \ttime/epoch: 3.08 \ttime_remaining:  0  hr: 50.5  min \ttime_stamp:  2018.11.16-10:33:55\n",
      "LSTM epoch: 18 RMSE-train: 0.7628298 RMSE-test 0.66511095 lr 0.0008 \ttime/epoch: 3.14 \ttime_remaining:  0  hr: 51.3  min \ttime_stamp:  2018.11.16-10:33:58\n",
      "LSTM epoch: 19 RMSE-train: 0.7830403 RMSE-test 1.2949507 lr 0.0008 \ttime/epoch: 3.06 \ttime_remaining:  0  hr: 50.1  min \ttime_stamp:  2018.11.16-10:34:01\n",
      "LSTM epoch: 20 RMSE-train: 0.7510607 RMSE-test 0.6287434 lr 0.0008 \ttime/epoch: 3.09 \ttime_remaining:  0  hr: 50.5  min \ttime_stamp:  2018.11.16-10:34:04\n",
      "LSTM epoch: 21 RMSE-train: 0.87492514 RMSE-test 0.7076816 lr 0.0008 \ttime/epoch: 3.03 \ttime_remaining:  0  hr: 49.5  min \ttime_stamp:  2018.11.16-10:34:07\n",
      "LSTM epoch: 22 RMSE-train: 0.81859505 RMSE-test 0.7731753 lr 0.0008 \ttime/epoch: 3.11 \ttime_remaining:  0  hr: 50.7  min \ttime_stamp:  2018.11.16-10:34:10\n",
      "LSTM epoch: 23 RMSE-train: 0.79201114 RMSE-test 0.9135397 lr 0.0008 \ttime/epoch: 3.1 \ttime_remaining:  0  hr: 50.4  min \ttime_stamp:  2018.11.16-10:34:13\n",
      "LSTM epoch: 24 RMSE-train: 0.77169776 RMSE-test 1.0433857 lr 0.0008 \ttime/epoch: 3.15 \ttime_remaining:  0  hr: 51.3  min \ttime_stamp:  2018.11.16-10:34:16\n",
      "LSTM epoch: 25 RMSE-train: 0.7521815 RMSE-test 1.2861129 lr 0.0008 \ttime/epoch: 3.17 \ttime_remaining:  0  hr: 51.5  min \ttime_stamp:  2018.11.16-10:34:20\n",
      "LSTM epoch: 26 RMSE-train: 0.73125625 RMSE-test 1.5513197 lr 0.0008 \ttime/epoch: 3.06 \ttime_remaining:  0  hr: 49.6  min \ttime_stamp:  2018.11.16-10:34:23\n",
      "LSTM epoch: 27 RMSE-train: 0.72364616 RMSE-test 1.4372643 lr 0.0008 \ttime/epoch: 3.19 \ttime_remaining:  0  hr: 51.7  min \ttime_stamp:  2018.11.16-10:34:26\n",
      "LSTM epoch: 28 RMSE-train: 0.71420527 RMSE-test 1.628474 lr 0.0008 \ttime/epoch: 3.06 \ttime_remaining:  0  hr: 49.6  min \ttime_stamp:  2018.11.16-10:34:29\n",
      "LSTM epoch: 29 RMSE-train: 0.7270096 RMSE-test 1.3891271 lr 0.0008 \ttime/epoch: 3.11 \ttime_remaining:  0  hr: 50.3  min \ttime_stamp:  2018.11.16-10:34:32\n",
      "LSTM epoch: 30 RMSE-train: 0.7175594 RMSE-test 1.3993031 lr 0.0008 \ttime/epoch: 3.11 \ttime_remaining:  0  hr: 50.3  min \ttime_stamp:  2018.11.16-10:34:35\n",
      "LSTM epoch: 31 RMSE-train: 0.77224994 RMSE-test 1.3373884 lr 0.0008 \ttime/epoch: 3.14 \ttime_remaining:  0  hr: 50.7  min \ttime_stamp:  2018.11.16-10:34:38\n",
      "LSTM epoch: 32 RMSE-train: 0.72694916 RMSE-test 1.606036 lr 0.0008 \ttime/epoch: 3.19 \ttime_remaining:  0  hr: 51.4  min \ttime_stamp:  2018.11.16-10:34:41\n",
      "LSTM epoch: 33 RMSE-train: 0.71071947 RMSE-test 1.3892202 lr 0.0008 \ttime/epoch: 3.11 \ttime_remaining:  0  hr: 50.1  min \ttime_stamp:  2018.11.16-10:34:45\n",
      "LSTM epoch: 34 RMSE-train: 0.7067302 RMSE-test 1.7861608 lr 0.0008 \ttime/epoch: 3.1 \ttime_remaining:  0  hr: 49.9  min \ttime_stamp:  2018.11.16-10:34:48\n",
      "LSTM epoch: 35 RMSE-train: 0.69286585 RMSE-test 1.1625557 lr 0.0008 \ttime/epoch: 3.12 \ttime_remaining:  0  hr: 50.2  min \ttime_stamp:  2018.11.16-10:34:51\n",
      "LSTM epoch: 36 RMSE-train: 0.72121906 RMSE-test 1.7836722 lr 0.0008 \ttime/epoch: 3.16 \ttime_remaining:  0  hr: 50.7  min \ttime_stamp:  2018.11.16-10:34:54\n",
      "LSTM epoch: 37 RMSE-train: 0.69581616 RMSE-test 1.7228444 lr 0.0008 \ttime/epoch: 3.13 \ttime_remaining:  0  hr: 50.2  min \ttime_stamp:  2018.11.16-10:34:57\n",
      "LSTM epoch: 38 RMSE-train: 0.7131157 RMSE-test 1.5330005 lr 0.0008 \ttime/epoch: 3.1 \ttime_remaining:  0  hr: 49.7  min \ttime_stamp:  2018.11.16-10:35:00\n",
      "LSTM epoch: 39 RMSE-train: 0.6974896 RMSE-test 1.4435945 lr 0.0008 \ttime/epoch: 3.05 \ttime_remaining:  0  hr: 48.9  min \ttime_stamp:  2018.11.16-10:35:03\n",
      "LSTM epoch: 40 RMSE-train: 0.6800011 RMSE-test 1.3825885 lr 0.0008 \ttime/epoch: 3.13 \ttime_remaining:  0  hr: 50.0  min \ttime_stamp:  2018.11.16-10:35:06\n",
      "LSTM epoch: 41 RMSE-train: 0.6847034 RMSE-test 1.5658705 lr 0.0008 \ttime/epoch: 3.08 \ttime_remaining:  0  hr: 49.2  min \ttime_stamp:  2018.11.16-10:35:09\n",
      "LSTM epoch: 42 RMSE-train: 0.67321247 RMSE-test 0.8042599 lr 0.0008 \ttime/epoch: 3.16 \ttime_remaining:  0  hr: 50.5  min \ttime_stamp:  2018.11.16-10:35:13\n",
      "LSTM epoch: 43 RMSE-train: 0.7199 RMSE-test 1.8537704 lr 0.0008 \ttime/epoch: 3.08 \ttime_remaining:  0  hr: 49.2  min \ttime_stamp:  2018.11.16-10:35:16\n",
      "LSTM epoch: 44 RMSE-train: 0.68875384 RMSE-test 0.90797573 lr 0.0008 \ttime/epoch: 3.03 \ttime_remaining:  0  hr: 48.2  min \ttime_stamp:  2018.11.16-10:35:19\n",
      "LSTM epoch: 45 RMSE-train: 0.7873367 RMSE-test 1.4093261 lr 0.0008 \ttime/epoch: 3.06 \ttime_remaining:  0  hr: 48.7  min \ttime_stamp:  2018.11.16-10:35:22\n",
      "LSTM epoch: 46 RMSE-train: 0.7063663 RMSE-test 1.2637398 lr 0.0008 \ttime/epoch: 3.04 \ttime_remaining:  0  hr: 48.4  min \ttime_stamp:  2018.11.16-10:35:25\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-1770e065708c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mitr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                 \u001b[0;31m## training ##\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                 \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m                 \u001b[0;31m#print(\"states:\",session.run(states))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 session.run([optimizer, update_op],\n",
      "\u001b[0;32m/notebooks/Sibur/utils_laj.py\u001b[0m in \u001b[0;36mbatch_generator\u001b[0;34m(x_train, y_train, batch_size, sequence_length, online, online_shift)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0mx_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0my_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0monline\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "    with tf.Session() as session:\n",
    "        tf.global_variables_initializer().run()\n",
    "\n",
    "        if Train == True:\n",
    "            #saver.restore(session, path_checkpoint)\n",
    "            #print(\"Model restored from file: %s\" % path_checkpoint)\n",
    "\n",
    "            cost = []\n",
    "            plot_x = []\n",
    "            plot_y1 = []\n",
    "            plot_y2 = []\n",
    "            iter_train = int(x_train.shape[0]/shift)\n",
    "            iter_test = int(x_test.shape[0]/shift)\n",
    "            print(\"Training set MSE\")\n",
    "            print(\"No epoches: \", epochs, \"No itr: \", iter_train)\n",
    "            __start = time.time()\n",
    "            for ep in range(epochs):\n",
    "                session.run(reset_op)\n",
    "                \n",
    "                h1 = []\n",
    "                t1 = []\n",
    "                   \n",
    "                for itr in range(iter_train):\n",
    "                    ## training ##\n",
    "                    batch_x, batch_y = next(training_generator)\n",
    "                    #print(\"states:\",session.run(states))\n",
    "                    session.run([optimizer, update_op],\n",
    "                                feed_dict={X: batch_x, Y: batch_y, keep_prob: 0.7, learning_rate_: learning_rate})\n",
    "                    h_i = h.eval(feed_dict={X: batch_x, Y: batch_y, keep_prob: 1.0, learning_rate_: learning_rate})\n",
    "                    cost.append(np.square(h_i))\n",
    "                    h1.append(h_i)\n",
    "\n",
    "                rmse_train = np.sqrt(np.mean(np.square(h1)))\n",
    "                \n",
    "                y_pred = []\n",
    "                for itr in range(iter_test):\n",
    "                    x_test_batch, y_test_batch = next(testing_generator)\n",
    "                    #print(\"states:\",session.run(states))\n",
    "                    h_i, u = session.run([h, update_op], feed_dict={X: x_test_batch, Y: y_test_batch, keep_prob: 1.0, learning_rate_: learning_rate})\n",
    "                    #session.run(update_op)\n",
    "                    t1.append(h_i)\n",
    "                \n",
    "                rmse_test = np.sqrt(np.mean(np.square(t1)))\n",
    "                \n",
    "                #rmse_train = session.run(RMSE, feed_dict={X: batch_x, Y: batch_y, keep_prob: 1.0})\n",
    "                rmse_test = session.run(RMSE, feed_dict={X: x_test_batch, Y: y_test_batch, keep_prob: 1.0})\n",
    "                plot_x.append(ep)\n",
    "                plot_y1.append(rmse_train)\n",
    "                plot_y2.append(rmse_test)\n",
    "                \n",
    "                #print(ep)\n",
    "\n",
    "                time_per_ep = (time.time() - __start)\n",
    "                time_remaining = ((epochs - ep) * time_per_ep) / 3600\n",
    "                print(\"LSTM\", \"epoch:\", ep, \"RMSE-train:\", rmse_train, \"RMSE-test\", rmse_test, \"lr\", learning_rate,\n",
    "                      \"\\ttime/epoch:\", round(time_per_ep, 2), \"\\ttime_remaining: \",\n",
    "                      int(time_remaining), \" hr:\", round((time_remaining % 1) * 60, 1), \" min\", \"\\ttime_stamp: \",\n",
    "                      datetime.datetime.now().strftime(\"%Y.%m.%d-%H:%M:%S\"))\n",
    "                __start = time.time()\n",
    "\n",
    "                if ep % 100 == 0 and ep != 0:\n",
    "                    save_path = saver.save(session, path_checkpoint)\n",
    "                    if os.path.exists(path_checkpoint + '.meta'):\n",
    "                        print(\"Model saved to file: %s\" % path_checkpoint)\n",
    "                    else:\n",
    "                        print(\"NOT SAVED!!!\", path_checkpoint)\n",
    "\n",
    "                if ep % 50 == 0 and ep != 0: \n",
    "                    plt.plot(plot_x, plot_y1, 'bo', plot_x, plot_y2, 'go')\n",
    "                    plt.show()\n",
    "                        \n",
    "                if ep % 100 == 0 and ep != 0: \n",
    "                    learning_rate = learning_rate / 2\n",
    "\n",
    "\n",
    "            save_path = saver.save(session, path_checkpoint)\n",
    "            if os.path.exists(path_checkpoint + '.meta'):\n",
    "                print(\"Model saved to file: %s\" % path_checkpoint)\n",
    "            else:\n",
    "                print(\"NOT SAVED!!!\", path_checkpoint)\n",
    "            plt.plot(plot_x, plot_y1, 'bo', plot_x, plot_y2, 'go')\n",
    "            plt.show()\n",
    "        else:\n",
    "            saver.restore(session, path_checkpoint)\n",
    "            print(\"Model restored from file: %s\" % path_checkpoint)\n",
    "            if Predict == True:\n",
    "                print(\"Prediction for submit...\")\n",
    "                x_predict = Xs\n",
    "                y_predict = np.zeros((Xs.shape[0],Xs.shape[1]))\n",
    "\n",
    "                predict_generator = batch_generator(x_predict, y_predict, batch_size, sequence_length,\n",
    "                                                       online=True, online_shift=shift)\n",
    "\n",
    "                full_prediction = []\n",
    "\n",
    "                iteration = int(x_predict.shape[0] / shift)\n",
    "                #print(\"iteration: %i, predgen %s\" % (iteration, predict_generator))\n",
    "                print(\"#of validation points:\", x_predict.shape[0], \"#datapoints covers from minibatch:\",\n",
    "                      batch_size * sequence_length, \"iterations/epoch\", iteration)\n",
    "\n",
    "                for itr in range(iteration):\n",
    "                    x_validate_batch, y_validate_batch = next(predict_generator)\n",
    "                    #print (itr)\n",
    "                    __y_pred, u = session.run([output, update_op], feed_dict={X: x_validate_batch, Y: y_validate_batch, keep_prob: 1.0})\n",
    "                    #session.run(update_op)\n",
    "                    for i in range(batch_size):\n",
    "                        full_prediction.append(__y_pred[i*sequence_length])\n",
    "                    #print(__y_pred.shape)\n",
    "                    \n",
    "                full_prediction = np.array(full_prediction)\n",
    "                full_prediction = full_prediction.ravel()\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_submit = full_prediction*std_y+mean_y\n",
    "sub_file = pd.DataFrame()\n",
    "sub_file[\"timestamp\"] = submit_X.index[-2872:]\n",
    "sub_file[\"target\"] = y_submit[-2872:]\n",
    "sub_file.to_csv('submit2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LB 0.4705"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
