{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import datetime\n",
    "from utils_laj import *\n",
    "from data_processing import get_SensorData, my_pca, series_to_supervised, lstm_sampling\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "today = datetime.date.today()\n",
    "TR_END = \"2017-12-31 23:00:00\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_checkpoint = './save/save_lstm/lstm_2_layers'\n",
    "input_path = './input_data'\n",
    "file = os.path.join(input_path,'sensors.csv')\n",
    "target_file = os.path.join(input_path,'coke_target.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/pandas/core/frame.py:3790: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  downcast=downcast, **kwargs)\n",
      "/usr/local/lib/python3.5/dist-packages/pandas/core/generic.py:5434: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._update_inplace(new_data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance [0.5383315  0.09938994 0.08926971 0.04085576 0.03484106 0.032776\n",
      " 0.02645778 0.01969716 0.01752129 0.01691421 0.01532406 0.01388007], Cumsum [0.5383315  0.63772144 0.72699115 0.76784692 0.80268798 0.83546398\n",
      " 0.86192176 0.88161892 0.89914021 0.91605442 0.93137848 0.94525856]\n"
     ]
    }
   ],
   "source": [
    "# Считываем данные из файлов, нормализуем и применяем метод главных комполнентов\n",
    "X, y, submit_X, mean_y, std_y = get_SensorData(file, target_file, nc=12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data X: (10400, 12), y (10400, 2), submit_X (13272, 12)\n"
     ]
    }
   ],
   "source": [
    "# Размерности датасета с известным целевым y и полного датасета\n",
    "print(\"Data X: %s, y %s, submit_X %s\" % (X.shape, y.shape, submit_X.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples length 7432\n",
      "Targets length 7432\n",
      "Samples length 2872\n",
      "Targets length 2872\n",
      "Samples length 2872\n",
      "Targets length 0\n",
      "Data x_train, y_train: ((7432, 48, 12),(7432, 48));  x_test, y_test: ((2872, 48, 12),(2872, 48)); Xs, ys: ((2872, 48, 12),None) \n"
     ]
    }
   ],
   "source": [
    "# выделим последние записи в размере контрольного датасета в тестовый датасет\n",
    "x1_train, x1_test, y1_train, y1_test = train_test_split(X, y, test_size=2920, random_state=42, shuffle=False)\n",
    "\n",
    "# Формируем сэмплы для LSTM по 48 рядов внахлест с шагом 1\n",
    "timesteps = 48\n",
    "lag = 1\n",
    "x_train, y_train = lstm_sampling(x1_train, y=y1_train['target'], timesteps=timesteps, lag=lag)\n",
    "x_test, y_test = lstm_sampling(x1_test, y=y1_test['target'], timesteps=timesteps, lag=lag)\n",
    "#yl = y['target'][timesteps:]\n",
    "\n",
    "# формируем данные сабмита по аналогичному принципу + берем 47 шагов из тестовых данных\n",
    "Xs, ys = lstm_sampling(submit_X[len(x_train)+len(x_test)+timesteps:], timesteps=timesteps, lag=lag)\n",
    "\n",
    "print(\"Data x_train, y_train: (%s,%s);  x_test, y_test: (%s,%s); Xs, ys: (%s,%s) \" % (x_train.shape, y_train.shape, x_test.shape, y_test.shape, Xs.shape, ys))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "    Train = False\n",
    "    Predict = True\n",
    "    plot = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "    batch_size = 359  # Batch size\n",
    "    shift = batch_size\n",
    "    #if Train == False: batch_size = 1\n",
    "\n",
    "    sequence_length = timesteps  # Number of steps\n",
    "    learning_rate = 4*10e-5  # 0.0001\n",
    "    epochs = 1000\n",
    "    ann_hidden = 16\n",
    "\n",
    "    n_channels = x_train.shape[2]\n",
    "\n",
    "    lstm_size = 48  # Number LSTM units\n",
    "    num_layers = 3  # 2  # Number of layers\n",
    "    alpha = 0 # regularization coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My Conv Shape: [None, 48, 12]\n"
     ]
    }
   ],
   "source": [
    "    X = tf.placeholder(tf.float32, [None, sequence_length, n_channels], name='inputs')\n",
    "    Y = tf.placeholder(tf.float32, [None, sequence_length], name='labels')\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    learning_rate_ = tf.placeholder(tf.float32, name='learning_rate')\n",
    "    is_train = tf.placeholder(dtype=tf.bool, shape=None, name=\"is_train\")\n",
    "\n",
    "    conv_last_layer = X\n",
    "\n",
    "    shape = conv_last_layer.get_shape().as_list()\n",
    "    print('My Conv Shape:',shape)\n",
    "    CNN_flat = tf.reshape(conv_last_layer, [-1, shape[1] * shape[2]])\n",
    "\n",
    "    dence_layer_1 = dense_layer(CNN_flat, size=sequence_length * n_channels, activation_fn=tf.nn.relu, batch_norm=False,\n",
    "                                phase=is_train, drop_out=True, keep_prob=keep_prob,\n",
    "                                scope=\"fc_1\")\n",
    "    lstm_input = tf.reshape(dence_layer_1, [-1, sequence_length, n_channels])\n",
    "\n",
    "    cell = get_RNNCell(['LSTM'] * num_layers, keep_prob=keep_prob, state_size=lstm_size)\n",
    "    init_state = cell.zero_state(batch_size, tf.float32)\n",
    "    #init_state = tf.Variable(cell.zero_states(batch_size, tf.float32), trainable=False)\n",
    "    rnn_output, states = tf.nn.dynamic_rnn(cell, lstm_input, dtype=tf.float32, initial_state=init_state)\n",
    "    #with tf.control_dependencies([state.assign(states)]):\n",
    "    #    output = tf.identity(output)\n",
    "    \n",
    "    stacked_rnn_output = tf.reshape(rnn_output, [-1, lstm_size])  # change the form into a tensor\n",
    "\n",
    "    dence_layer_2 = dense_layer(stacked_rnn_output, size=ann_hidden, activation_fn=tf.nn.relu, batch_norm=False,\n",
    "                                phase=is_train, drop_out=True, keep_prob=keep_prob,\n",
    "                                scope=\"fc_2\")\n",
    "    \n",
    "    dence_layer_3 = dense_layer(dence_layer_2, size=ann_hidden, activation_fn=tf.nn.relu, batch_norm=False,\n",
    "                                phase=is_train, drop_out=True, keep_prob=keep_prob,\n",
    "                                scope=\"fc_2_2\")\n",
    "\n",
    "    output = dense_layer(dence_layer_3, size=1, activation_fn=None, batch_norm=False, phase=is_train, drop_out=False,\n",
    "                         keep_prob=keep_prob,\n",
    "                         scope=\"fc_3_output\")\n",
    "\n",
    "    prediction = tf.reshape(output, [-1])\n",
    "    y_flat = tf.reshape(Y, [-1])\n",
    "\n",
    "    h = prediction - y_flat\n",
    "    \n",
    "    tv = tf.trainable_variables()\n",
    "    regularization_cost = tf.reduce_sum([ tf.nn.l2_loss(v) for v in tv ])\n",
    "\n",
    "    cost_function = tf.reduce_sum(tf.square(h)) + alpha*regularization_cost\n",
    "    RMSE = tf.sqrt(tf.reduce_mean(tf.square(h)))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate_).minimize(cost_function)\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    training_generator = batch_generator(x_train, y_train, batch_size, sequence_length, online=True, online_shift=shift)\n",
    "    testing_generator = batch_generator(x_test, y_test, batch_size, sequence_length, online=True, online_shift=shift)\n",
    "    #print(len(list(training_generator)))\n",
    "\n",
    "    if Train: model_summary(learning_rate=learning_rate, batch_size=batch_size, lstm_layers=num_layers,\n",
    "                            lstm_layer_size=lstm_size, fc_layer_size=ann_hidden, sequence_length=sequence_length,\n",
    "                            n_channels=n_channels, path_checkpoint=path_checkpoint, spacial_note='')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./save/save_lstm/lstm_2_layers\n",
      "Model restored from file: ./save/save_lstm/lstm_2_layers\n",
      "Prediction for submit...\n",
      "#of validation points: 2872 #datapoints covers from minibatch: 17232 iterations/epoch 8\n"
     ]
    }
   ],
   "source": [
    "    with tf.Session() as session:\n",
    "        tf.global_variables_initializer().run()\n",
    "\n",
    "        if Train == True:\n",
    "            #saver.restore(session, path_checkpoint)\n",
    "            print(\"Model restored from file: %s\" % path_checkpoint)\n",
    "\n",
    "            cost = []\n",
    "            plot_x = []\n",
    "            plot_y1 = []\n",
    "            plot_y2 = []\n",
    "            iter_train = int(x_train.shape[0]/shift)\n",
    "            iter_test = int(x_test.shape[0]/shift)\n",
    "            print(\"Training set MSE\")\n",
    "            print(\"No epoches: \", epochs, \"No itr: \", iter_train)\n",
    "            __start = time.time()\n",
    "            for ep in range(epochs):\n",
    "                h1 = []\n",
    "                t1 = []\n",
    "                \n",
    "                for itr in range(iter_train):\n",
    "                    ## training ##\n",
    "                    batch_x, batch_y = next(training_generator)\n",
    "                    session.run(optimizer,\n",
    "                                feed_dict={X: batch_x, Y: batch_y, keep_prob: 0.6, learning_rate_: learning_rate})\n",
    "                    h_i = h.eval(feed_dict={X: batch_x, Y: batch_y, keep_prob: 1.0, learning_rate_: learning_rate})\n",
    "                    cost.append(np.square(h_i))\n",
    "                    h1.append(h_i)\n",
    "\n",
    "                rmse_train = np.sqrt(np.mean(np.square(h1)))\n",
    "                \n",
    "                y_pred = []\n",
    "                for itr in range(iter_test):\n",
    "                    x_test_batch, y_test_batch = next(testing_generator)\n",
    "                    h_i = h.eval(feed_dict={X: x_test_batch, Y: y_test_batch, keep_prob: 1.0, learning_rate_: learning_rate})\n",
    "                    t1.append(h_i)\n",
    "                \n",
    "                rmse_test = np.sqrt(np.mean(np.square(t1)))\n",
    "                \n",
    "                #rmse_train = session.run(RMSE, feed_dict={X: batch_x, Y: batch_y, keep_prob: 1.0})\n",
    "                rmse_test = session.run(RMSE, feed_dict={X: x_test_batch, Y: y_test_batch, keep_prob: 1.0})\n",
    "                plot_x.append(ep)\n",
    "                plot_y1.append(rmse_train)\n",
    "                plot_y2.append(rmse_test)\n",
    "                \n",
    "                #print(ep)\n",
    "\n",
    "                time_per_ep = (time.time() - __start)\n",
    "                time_remaining = ((epochs - ep) * time_per_ep) / 3600\n",
    "                print(\"LSTM\", \"epoch:\", ep, \"RMSE-train:\", rmse_train, \"RMSE-test\", rmse_test, \"lr\", learning_rate,\n",
    "                      \"\\ttime/epoch:\", round(time_per_ep, 2), \"\\ttime_remaining: \",\n",
    "                      int(time_remaining), \" hr:\", round((time_remaining % 1) * 60, 1), \" min\", \"\\ttime_stamp: \",\n",
    "                      datetime.datetime.now().strftime(\"%Y.%m.%d-%H:%M:%S\"))\n",
    "                __start = time.time()\n",
    "\n",
    "                if ep % 10 == 0 and ep != 0:\n",
    "                    save_path = saver.save(session, path_checkpoint)\n",
    "                    if os.path.exists(path_checkpoint + '.meta'):\n",
    "                        print(\"Model saved to file: %s\" % path_checkpoint)\n",
    "                    else:\n",
    "                        print(\"NOT SAVED!!!\", path_checkpoint)\n",
    "\n",
    "                if ep % 50 == 0 and ep != 0: \n",
    "                    plt.plot(plot_x, plot_y1, 'bo', plot_x, plot_y2, 'go')\n",
    "                    plt.show()\n",
    "                        \n",
    "                if ep % 200 == 0 and ep != 0: \n",
    "                    learning_rate = learning_rate / 2\n",
    "\n",
    "\n",
    "            save_path = saver.save(session, path_checkpoint)\n",
    "            if os.path.exists(path_checkpoint + '.meta'):\n",
    "                print(\"Model saved to file: %s\" % path_checkpoint)\n",
    "            else:\n",
    "                print(\"NOT SAVED!!!\", path_checkpoint)\n",
    "            plt.plot(plot_x, plot_y1, 'bo', plot_x, plot_y2, 'go')\n",
    "            plt.show()\n",
    "        else:\n",
    "            saver.restore(session, path_checkpoint)\n",
    "            print(\"Model restored from file: %s\" % path_checkpoint)\n",
    "            if Predict == True:\n",
    "                print(\"Prediction for submit...\")\n",
    "                x_predict = Xs\n",
    "                y_predict = np.zeros((Xs.shape[0],Xs.shape[1]))\n",
    "\n",
    "                predict_generator = batch_generator(x_predict, y_predict, batch_size, sequence_length,\n",
    "                                                       online=True, online_shift=shift)\n",
    "\n",
    "                full_prediction = []\n",
    "\n",
    "                iteration = int(x_predict.shape[0] / shift)\n",
    "                #print(\"iteration: %i, predgen %s\" % (iteration, predict_generator))\n",
    "                print(\"#of validation points:\", x_predict.shape[0], \"#datapoints covers from minibatch:\",\n",
    "                      batch_size * sequence_length, \"iterations/epoch\", iteration)\n",
    "\n",
    "                for itr in range(iteration):\n",
    "                    x_validate_batch, y_validate_batch = next(predict_generator)\n",
    "                    #print (itr)\n",
    "                    __y_pred = session.run(output, feed_dict={X: x_validate_batch, Y: y_validate_batch, keep_prob: 1.0})\n",
    "                    \n",
    "                    for i in range(batch_size):\n",
    "                        full_prediction.append(__y_pred[i*sequence_length])\n",
    "                    #print(__y_pred.shape)\n",
    "                    \n",
    "                full_prediction = np.array(full_prediction)\n",
    "                full_prediction = full_prediction.ravel()\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(359, 48, 12)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_validate_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.08393462, 0.0806639 , 0.05835484, ..., 0.1992277 , 0.19978933,\n",
       "       0.19981389], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2872,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_prediction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "shft = x_train.shape[0]+x_test.shape[0]+2*sequence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.159007111780772"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_submit = full_prediction*std_y+mean_y\n",
    "sub_file = pd.DataFrame()\n",
    "sub_file[\"timestamp\"] = submit_X.index[shft:]\n",
    "sub_file[\"target\"] = y_submit\n",
    "sub_file.to_csv('submit2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LB 0.4705"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
