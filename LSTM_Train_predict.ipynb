{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import datetime\n",
    "from utils_laj import *\n",
    "from data_processing import get_SensorData, my_pca, series_to_supervised, lstm_sampling\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "today = datetime.date.today()\n",
    "TR_END = \"2017-12-31 23:00:00\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_checkpoint = './save/save_lstm/lstm_2_layers'\n",
    "input_path = './input_data'\n",
    "file = os.path.join(input_path,'sensors.csv')\n",
    "target_file = os.path.join(input_path,'coke_target.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/pandas/core/frame.py:3790: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  downcast=downcast, **kwargs)\n",
      "/usr/local/lib/python3.5/dist-packages/pandas/core/generic.py:5434: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._update_inplace(new_data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance [0.5383315  0.09938994 0.08926971 0.04085576 0.03484106 0.032776\n",
      " 0.02645778 0.01969716 0.01752129 0.01691421 0.01532406 0.01388007], Cumsum [0.5383315  0.63772144 0.72699115 0.76784692 0.80268798 0.83546398\n",
      " 0.86192176 0.88161892 0.89914021 0.91605442 0.93137848 0.94525856]\n"
     ]
    }
   ],
   "source": [
    "# Считываем данные из файлов, нормализуем и применяем метод главных комполнентов\n",
    "X, y, submit_X, mean_y, std_y = get_SensorData(file, target_file, nc=12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data X: (10400, 12), y (10400, 2), submit_X (13272, 12)\n"
     ]
    }
   ],
   "source": [
    "# Размерности датасета с известным целевым y и полного датасета\n",
    "print(\"Data X: %s, y %s, submit_X %s\" % (X.shape, y.shape, submit_X.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples length 7432\n",
      "Targets length 7432\n",
      "Samples length 2872\n",
      "Targets length 2872\n",
      "Samples length 2872\n",
      "Targets length 0\n",
      "Data x_train, y_train: ((7432, 48, 12),(7432, 48));  x_test, y_test: ((2872, 48, 12),(2872, 48)); Xs, ys: ((2872, 48, 12),None) \n"
     ]
    }
   ],
   "source": [
    "# выделим последние записи в размере контрольного датасета в тестовый датасет\n",
    "x1_train, x1_test, y1_train, y1_test = train_test_split(X, y, test_size=2920, random_state=42, shuffle=False)\n",
    "\n",
    "# Формируем сэмплы для LSTM по 48 рядов внахлест с шагом 1\n",
    "timesteps = 48\n",
    "lag = 1\n",
    "x_train, y_train = lstm_sampling(x1_train, y=y1_train['target'], timesteps=timesteps, lag=lag)\n",
    "x_test, y_test = lstm_sampling(x1_test, y=y1_test['target'], timesteps=timesteps, lag=lag)\n",
    "#yl = y['target'][timesteps:]\n",
    "\n",
    "# формируем данные сабмита по аналогичному принципу + берем 47 шагов из тестовых данных\n",
    "Xs, ys = lstm_sampling(submit_X[len(x_train)+len(x_test)+timesteps:], timesteps=timesteps, lag=lag)\n",
    "\n",
    "print(\"Data x_train, y_train: (%s,%s);  x_test, y_test: (%s,%s); Xs, ys: (%s,%s) \" % (x_train.shape, y_train.shape, x_test.shape, y_test.shape, Xs.shape, ys))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "    Train = True\n",
    "    Predict = True\n",
    "    plot = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "    batch_size = 359  # Batch size\n",
    "    shift = batch_size\n",
    "    #if Train == False: batch_size = 1\n",
    "\n",
    "    sequence_length = timesteps  # Number of steps\n",
    "    learning_rate = 4*10e-5  # 0.0001\n",
    "    epochs = 1000\n",
    "    ann_hidden = 16\n",
    "\n",
    "    n_channels = x_train.shape[2]\n",
    "\n",
    "    lstm_size = 48  # Number LSTM units\n",
    "    num_layers = 3  # 2  # Number of layers\n",
    "    alpha = 0 # regularization coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My Conv Shape: [None, 48, 12]\n"
     ]
    }
   ],
   "source": [
    "    X = tf.placeholder(tf.float32, [None, sequence_length, n_channels], name='inputs')\n",
    "    Y = tf.placeholder(tf.float32, [None, sequence_length], name='labels')\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    learning_rate_ = tf.placeholder(tf.float32, name='learning_rate')\n",
    "    is_train = tf.placeholder(dtype=tf.bool, shape=None, name=\"is_train\")\n",
    "\n",
    "    conv_last_layer = X\n",
    "\n",
    "    shape = conv_last_layer.get_shape().as_list()\n",
    "    print('My Conv Shape:',shape)\n",
    "    CNN_flat = tf.reshape(conv_last_layer, [-1, shape[1] * shape[2]])\n",
    "\n",
    "    dence_layer_1 = dense_layer(CNN_flat, size=sequence_length * n_channels, activation_fn=tf.nn.relu, batch_norm=False,\n",
    "                                phase=is_train, drop_out=True, keep_prob=keep_prob,\n",
    "                                scope=\"fc_1\")\n",
    "    lstm_input = tf.reshape(dence_layer_1, [-1, sequence_length, n_channels])\n",
    "\n",
    "    cell = get_RNNCell(['LSTM'] * num_layers, keep_prob=keep_prob, state_size=lstm_size)\n",
    "    init_state = cell.zero_state(batch_size, tf.float32)\n",
    "    #init_state = tf.Variable(cell.zero_states(batch_size, tf.float32), trainable=False)\n",
    "    rnn_output, states = tf.nn.dynamic_rnn(cell, lstm_input, dtype=tf.float32, initial_state=init_state)\n",
    "    #with tf.control_dependencies([state.assign(states)]):\n",
    "    #    output = tf.identity(output)\n",
    "    \n",
    "    stacked_rnn_output = tf.reshape(rnn_output, [-1, lstm_size])  # change the form into a tensor\n",
    "\n",
    "    dence_layer_2 = dense_layer(stacked_rnn_output, size=ann_hidden, activation_fn=tf.nn.relu, batch_norm=False,\n",
    "                                phase=is_train, drop_out=True, keep_prob=keep_prob,\n",
    "                                scope=\"fc_2\")\n",
    "    \n",
    "    dence_layer_3 = dense_layer(dence_layer_2, size=ann_hidden, activation_fn=tf.nn.relu, batch_norm=False,\n",
    "                                phase=is_train, drop_out=True, keep_prob=keep_prob,\n",
    "                                scope=\"fc_2_2\")\n",
    "\n",
    "    output = dense_layer(dence_layer_3, size=1, activation_fn=None, batch_norm=False, phase=is_train, drop_out=False,\n",
    "                         keep_prob=keep_prob,\n",
    "                         scope=\"fc_3_output\")\n",
    "\n",
    "    prediction = tf.reshape(output, [-1])\n",
    "    y_flat = tf.reshape(Y, [-1])\n",
    "\n",
    "    h = prediction - y_flat\n",
    "    \n",
    "    tv = tf.trainable_variables()\n",
    "    regularization_cost = tf.reduce_sum([ tf.nn.l2_loss(v) for v in tv ])\n",
    "\n",
    "    cost_function = tf.reduce_sum(tf.square(h)) + alpha*regularization_cost\n",
    "    RMSE = tf.sqrt(tf.reduce_mean(tf.square(h)))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate_).minimize(cost_function)\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    training_generator = batch_generator(x_train, y_train, batch_size, sequence_length, online=True, online_shift=shift)\n",
    "    testing_generator = batch_generator(x_test, y_test, batch_size, sequence_length, online=True, online_shift=shift)\n",
    "    #print(len(list(training_generator)))\n",
    "\n",
    "    if Train: model_summary(learning_rate=learning_rate, batch_size=batch_size, lstm_layers=num_layers,\n",
    "                            lstm_layer_size=lstm_size, fc_layer_size=ann_hidden, sequence_length=sequence_length,\n",
    "                            n_channels=n_channels, path_checkpoint=path_checkpoint, spacial_note='')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model restored from file: ./save/save_lstm/lstm_2_layers\n",
      "Training set MSE\n",
      "No epoches:  1000 No itr:  20\n",
      "LSTM epoch: 0 RMSE-train: 0.9299958 RMSE-test 0.6088169 lr 0.0004 \ttime/epoch: 4.94 \ttime_remaining:  1  hr: 22.3  min \ttime_stamp:  2018.11.15-11:35:38\n",
      "LSTM epoch: 1 RMSE-train: 0.92781144 RMSE-test 0.6102699 lr 0.0004 \ttime/epoch: 3.93 \ttime_remaining:  1  hr: 5.4  min \ttime_stamp:  2018.11.15-11:35:42\n",
      "LSTM epoch: 2 RMSE-train: 0.9270182 RMSE-test 0.612442 lr 0.0004 \ttime/epoch: 3.88 \ttime_remaining:  1  hr: 4.5  min \ttime_stamp:  2018.11.15-11:35:46\n",
      "LSTM epoch: 3 RMSE-train: 0.92648226 RMSE-test 0.6137757 lr 0.0004 \ttime/epoch: 3.84 \ttime_remaining:  1  hr: 3.8  min \ttime_stamp:  2018.11.15-11:35:50\n",
      "LSTM epoch: 4 RMSE-train: 0.9260904 RMSE-test 0.61510617 lr 0.0004 \ttime/epoch: 3.93 \ttime_remaining:  1  hr: 5.2  min \ttime_stamp:  2018.11.15-11:35:54\n",
      "LSTM epoch: 5 RMSE-train: 0.9253828 RMSE-test 0.61652434 lr 0.0004 \ttime/epoch: 3.85 \ttime_remaining:  1  hr: 3.8  min \ttime_stamp:  2018.11.15-11:35:57\n",
      "LSTM epoch: 6 RMSE-train: 0.92386055 RMSE-test 0.6173414 lr 0.0004 \ttime/epoch: 3.87 \ttime_remaining:  1  hr: 4.1  min \ttime_stamp:  2018.11.15-11:36:01\n",
      "LSTM epoch: 7 RMSE-train: 0.9210349 RMSE-test 0.61763257 lr 0.0004 \ttime/epoch: 3.92 \ttime_remaining:  1  hr: 4.8  min \ttime_stamp:  2018.11.15-11:36:05\n",
      "LSTM epoch: 8 RMSE-train: 0.9194339 RMSE-test 0.6112713 lr 0.0004 \ttime/epoch: 3.82 \ttime_remaining:  1  hr: 3.2  min \ttime_stamp:  2018.11.15-11:36:09\n",
      "LSTM epoch: 9 RMSE-train: 0.91398424 RMSE-test 0.60460466 lr 0.0004 \ttime/epoch: 3.93 \ttime_remaining:  1  hr: 4.8  min \ttime_stamp:  2018.11.15-11:36:13\n",
      "LSTM epoch: 10 RMSE-train: 0.9148028 RMSE-test 0.6036125 lr 0.0004 \ttime/epoch: 3.89 \ttime_remaining:  1  hr: 4.3  min \ttime_stamp:  2018.11.15-11:36:17\n",
      "Model saved to file: ./save/save_lstm/lstm_2_layers\n",
      "LSTM epoch: 11 RMSE-train: 0.9089801 RMSE-test 0.5882023 lr 0.0004 \ttime/epoch: 4.12 \ttime_remaining:  1  hr: 7.9  min \ttime_stamp:  2018.11.15-11:36:21\n",
      "LSTM epoch: 12 RMSE-train: 0.90307957 RMSE-test 0.583882 lr 0.0004 \ttime/epoch: 3.9 \ttime_remaining:  1  hr: 4.3  min \ttime_stamp:  2018.11.15-11:36:25\n",
      "LSTM epoch: 13 RMSE-train: 0.9008352 RMSE-test 0.5782922 lr 0.0004 \ttime/epoch: 3.9 \ttime_remaining:  1  hr: 4.2  min \ttime_stamp:  2018.11.15-11:36:29\n",
      "LSTM epoch: 14 RMSE-train: 0.8944293 RMSE-test 0.5679954 lr 0.0004 \ttime/epoch: 3.9 \ttime_remaining:  1  hr: 4.1  min \ttime_stamp:  2018.11.15-11:36:33\n",
      "LSTM epoch: 15 RMSE-train: 0.8891597 RMSE-test 0.57022285 lr 0.0004 \ttime/epoch: 3.91 \ttime_remaining:  1  hr: 4.2  min \ttime_stamp:  2018.11.15-11:36:37\n",
      "LSTM epoch: 16 RMSE-train: 0.8890602 RMSE-test 0.55656165 lr 0.0004 \ttime/epoch: 3.81 \ttime_remaining:  1  hr: 2.5  min \ttime_stamp:  2018.11.15-11:36:40\n",
      "LSTM epoch: 17 RMSE-train: 0.8788283 RMSE-test 0.56043124 lr 0.0004 \ttime/epoch: 3.93 \ttime_remaining:  1  hr: 4.4  min \ttime_stamp:  2018.11.15-11:36:44\n",
      "LSTM epoch: 18 RMSE-train: 0.88343626 RMSE-test 0.554314 lr 0.0004 \ttime/epoch: 3.85 \ttime_remaining:  1  hr: 2.9  min \ttime_stamp:  2018.11.15-11:36:48\n",
      "LSTM epoch: 19 RMSE-train: 0.88976574 RMSE-test 0.5533429 lr 0.0004 \ttime/epoch: 3.95 \ttime_remaining:  1  hr: 4.6  min \ttime_stamp:  2018.11.15-11:36:52\n",
      "LSTM epoch: 20 RMSE-train: 0.8915495 RMSE-test 0.55558866 lr 0.0004 \ttime/epoch: 3.9 \ttime_remaining:  1  hr: 3.8  min \ttime_stamp:  2018.11.15-11:36:56\n",
      "Model saved to file: ./save/save_lstm/lstm_2_layers\n",
      "LSTM epoch: 21 RMSE-train: 0.8748474 RMSE-test 0.5468316 lr 0.0004 \ttime/epoch: 4.0 \ttime_remaining:  1  hr: 5.2  min \ttime_stamp:  2018.11.15-11:37:00\n",
      "LSTM epoch: 22 RMSE-train: 0.8720879 RMSE-test 0.55782247 lr 0.0004 \ttime/epoch: 3.92 \ttime_remaining:  1  hr: 3.9  min \ttime_stamp:  2018.11.15-11:37:04\n",
      "LSTM epoch: 23 RMSE-train: 0.87134594 RMSE-test 0.55005735 lr 0.0004 \ttime/epoch: 3.83 \ttime_remaining:  1  hr: 2.4  min \ttime_stamp:  2018.11.15-11:37:08\n",
      "LSTM epoch: 24 RMSE-train: 0.86288965 RMSE-test 0.55363643 lr 0.0004 \ttime/epoch: 3.95 \ttime_remaining:  1  hr: 4.3  min \ttime_stamp:  2018.11.15-11:37:12\n",
      "LSTM epoch: 25 RMSE-train: 0.8620067 RMSE-test 0.5474977 lr 0.0004 \ttime/epoch: 3.97 \ttime_remaining:  1  hr: 4.5  min \ttime_stamp:  2018.11.15-11:37:16\n",
      "LSTM epoch: 26 RMSE-train: 0.8538323 RMSE-test 0.55210996 lr 0.0004 \ttime/epoch: 3.89 \ttime_remaining:  1  hr: 3.2  min \ttime_stamp:  2018.11.15-11:37:20\n",
      "LSTM epoch: 27 RMSE-train: 0.8522219 RMSE-test 0.55031437 lr 0.0004 \ttime/epoch: 3.9 \ttime_remaining:  1  hr: 3.3  min \ttime_stamp:  2018.11.15-11:37:24\n",
      "LSTM epoch: 28 RMSE-train: 0.84430885 RMSE-test 0.5497694 lr 0.0004 \ttime/epoch: 3.93 \ttime_remaining:  1  hr: 3.7  min \ttime_stamp:  2018.11.15-11:37:28\n",
      "LSTM epoch: 29 RMSE-train: 0.83487946 RMSE-test 0.54898906 lr 0.0004 \ttime/epoch: 3.96 \ttime_remaining:  1  hr: 4.1  min \ttime_stamp:  2018.11.15-11:37:31\n",
      "LSTM epoch: 30 RMSE-train: 0.82691133 RMSE-test 0.55861735 lr 0.0004 \ttime/epoch: 3.93 \ttime_remaining:  1  hr: 3.6  min \ttime_stamp:  2018.11.15-11:37:35\n",
      "Model saved to file: ./save/save_lstm/lstm_2_layers\n",
      "LSTM epoch: 31 RMSE-train: 0.8238276 RMSE-test 0.54137313 lr 0.0004 \ttime/epoch: 4.07 \ttime_remaining:  1  hr: 5.8  min \ttime_stamp:  2018.11.15-11:37:39\n",
      "LSTM epoch: 32 RMSE-train: 0.8293778 RMSE-test 0.54670846 lr 0.0004 \ttime/epoch: 3.95 \ttime_remaining:  1  hr: 3.8  min \ttime_stamp:  2018.11.15-11:37:43\n",
      "LSTM epoch: 33 RMSE-train: 0.8095295 RMSE-test 0.5577108 lr 0.0004 \ttime/epoch: 3.9 \ttime_remaining:  1  hr: 2.8  min \ttime_stamp:  2018.11.15-11:37:47\n",
      "LSTM epoch: 34 RMSE-train: 0.8046145 RMSE-test 0.5710581 lr 0.0004 \ttime/epoch: 3.93 \ttime_remaining:  1  hr: 3.3  min \ttime_stamp:  2018.11.15-11:37:51\n",
      "LSTM epoch: 35 RMSE-train: 0.79331917 RMSE-test 0.5646308 lr 0.0004 \ttime/epoch: 3.95 \ttime_remaining:  1  hr: 3.5  min \ttime_stamp:  2018.11.15-11:37:55\n",
      "LSTM epoch: 36 RMSE-train: 0.78587335 RMSE-test 0.557503 lr 0.0004 \ttime/epoch: 3.88 \ttime_remaining:  1  hr: 2.4  min \ttime_stamp:  2018.11.15-11:37:59\n",
      "LSTM epoch: 37 RMSE-train: 0.77053905 RMSE-test 0.5456726 lr 0.0004 \ttime/epoch: 3.88 \ttime_remaining:  1  hr: 2.3  min \ttime_stamp:  2018.11.15-11:38:03\n",
      "LSTM epoch: 38 RMSE-train: 0.77152234 RMSE-test 0.5508142 lr 0.0004 \ttime/epoch: 3.89 \ttime_remaining:  1  hr: 2.4  min \ttime_stamp:  2018.11.15-11:38:07\n",
      "LSTM epoch: 39 RMSE-train: 0.7661335 RMSE-test 0.5514024 lr 0.0004 \ttime/epoch: 3.91 \ttime_remaining:  1  hr: 2.6  min \ttime_stamp:  2018.11.15-11:38:11\n",
      "LSTM epoch: 40 RMSE-train: 0.76828074 RMSE-test 0.55570674 lr 0.0004 \ttime/epoch: 3.94 \ttime_remaining:  1  hr: 3.0  min \ttime_stamp:  2018.11.15-11:38:15\n",
      "Model saved to file: ./save/save_lstm/lstm_2_layers\n",
      "LSTM epoch: 41 RMSE-train: 0.7469871 RMSE-test 0.5408755 lr 0.0004 \ttime/epoch: 4.02 \ttime_remaining:  1  hr: 4.3  min \ttime_stamp:  2018.11.15-11:38:19\n",
      "LSTM epoch: 42 RMSE-train: 0.7398703 RMSE-test 0.55457044 lr 0.0004 \ttime/epoch: 3.86 \ttime_remaining:  1  hr: 1.6  min \ttime_stamp:  2018.11.15-11:38:23\n",
      "LSTM epoch: 43 RMSE-train: 0.7381848 RMSE-test 0.54847836 lr 0.0004 \ttime/epoch: 3.88 \ttime_remaining:  1  hr: 2.0  min \ttime_stamp:  2018.11.15-11:38:27\n",
      "LSTM epoch: 44 RMSE-train: 0.72109604 RMSE-test 0.5452092 lr 0.0004 \ttime/epoch: 3.88 \ttime_remaining:  1  hr: 1.8  min \ttime_stamp:  2018.11.15-11:38:30\n",
      "LSTM epoch: 45 RMSE-train: 0.7042166 RMSE-test 0.54809153 lr 0.0004 \ttime/epoch: 3.91 \ttime_remaining:  1  hr: 2.2  min \ttime_stamp:  2018.11.15-11:38:34\n",
      "LSTM epoch: 46 RMSE-train: 0.71745634 RMSE-test 0.54679847 lr 0.0004 \ttime/epoch: 3.85 \ttime_remaining:  1  hr: 1.2  min \ttime_stamp:  2018.11.15-11:38:38\n",
      "LSTM epoch: 47 RMSE-train: 0.722302 RMSE-test 0.54146737 lr 0.0004 \ttime/epoch: 3.88 \ttime_remaining:  1  hr: 1.6  min \ttime_stamp:  2018.11.15-11:38:42\n",
      "LSTM epoch: 48 RMSE-train: 0.7543813 RMSE-test 0.56099623 lr 0.0004 \ttime/epoch: 3.93 \ttime_remaining:  1  hr: 2.4  min \ttime_stamp:  2018.11.15-11:38:46\n",
      "LSTM epoch: 49 RMSE-train: 0.69781387 RMSE-test 0.54830796 lr 0.0004 \ttime/epoch: 3.96 \ttime_remaining:  1  hr: 2.8  min \ttime_stamp:  2018.11.15-11:38:50\n"
     ]
    }
   ],
   "source": [
    "    with tf.Session() as session:\n",
    "        tf.global_variables_initializer().run()\n",
    "\n",
    "        if Train == True:\n",
    "            #saver.restore(session, path_checkpoint)\n",
    "            print(\"Model restored from file: %s\" % path_checkpoint)\n",
    "\n",
    "            cost = []\n",
    "            plot_x = []\n",
    "            plot_y1 = []\n",
    "            plot_y2 = []\n",
    "            iter_train = int(x_train.shape[0]/shift)\n",
    "            iter_test = int(x_test.shape[0]/shift)\n",
    "            print(\"Training set MSE\")\n",
    "            print(\"No epoches: \", epochs, \"No itr: \", iter_train)\n",
    "            __start = time.time()\n",
    "            for ep in range(epochs):\n",
    "                h1 = []\n",
    "                t1 = []\n",
    "                \n",
    "                for itr in range(iter_train):\n",
    "                    ## training ##\n",
    "                    batch_x, batch_y = next(training_generator)\n",
    "                    session.run(optimizer,\n",
    "                                feed_dict={X: batch_x, Y: batch_y, keep_prob: 0.6, learning_rate_: learning_rate})\n",
    "                    h_i = h.eval(feed_dict={X: batch_x, Y: batch_y, keep_prob: 1.0, learning_rate_: learning_rate})\n",
    "                    cost.append(np.square(h_i))\n",
    "                    h1.append(h_i)\n",
    "\n",
    "                rmse_train = np.sqrt(np.mean(np.square(h1)))\n",
    "                \n",
    "                y_pred = []\n",
    "                for itr in range(iter_test):\n",
    "                    x_test_batch, y_test_batch = next(testing_generator)\n",
    "                    h_i = h.eval(feed_dict={X: x_test_batch, Y: y_test_batch, keep_prob: 1.0, learning_rate_: learning_rate})\n",
    "                    t1.append(h_i)\n",
    "                \n",
    "                rmse_test = np.sqrt(np.mean(np.square(t1)))\n",
    "                \n",
    "                #rmse_train = session.run(RMSE, feed_dict={X: batch_x, Y: batch_y, keep_prob: 1.0})\n",
    "                rmse_test = session.run(RMSE, feed_dict={X: x_test_batch, Y: y_test_batch, keep_prob: 1.0})\n",
    "                plot_x.append(ep)\n",
    "                plot_y1.append(rmse_train)\n",
    "                plot_y2.append(rmse_test)\n",
    "                \n",
    "                #print(ep)\n",
    "\n",
    "                time_per_ep = (time.time() - __start)\n",
    "                time_remaining = ((epochs - ep) * time_per_ep) / 3600\n",
    "                print(\"LSTM\", \"epoch:\", ep, \"RMSE-train:\", rmse_train, \"RMSE-test\", rmse_test, \"lr\", learning_rate,\n",
    "                      \"\\ttime/epoch:\", round(time_per_ep, 2), \"\\ttime_remaining: \",\n",
    "                      int(time_remaining), \" hr:\", round((time_remaining % 1) * 60, 1), \" min\", \"\\ttime_stamp: \",\n",
    "                      datetime.datetime.now().strftime(\"%Y.%m.%d-%H:%M:%S\"))\n",
    "                __start = time.time()\n",
    "\n",
    "                if ep % 10 == 0 and ep != 0:\n",
    "                    save_path = saver.save(session, path_checkpoint)\n",
    "                    if os.path.exists(path_checkpoint + '.meta'):\n",
    "                        print(\"Model saved to file: %s\" % path_checkpoint)\n",
    "                    else:\n",
    "                        print(\"NOT SAVED!!!\", path_checkpoint)\n",
    "\n",
    "                if ep % 50 == 0 and ep != 0: \n",
    "                    plt.plot(plot_x, plot_y1, 'bo', plot_x, plot_y2, 'go')\n",
    "                    plt.show()\n",
    "                        \n",
    "                if ep % 200 == 0 and ep != 0: \n",
    "                    learning_rate = learning_rate / 2\n",
    "\n",
    "\n",
    "            save_path = saver.save(session, path_checkpoint)\n",
    "            if os.path.exists(path_checkpoint + '.meta'):\n",
    "                print(\"Model saved to file: %s\" % path_checkpoint)\n",
    "            else:\n",
    "                print(\"NOT SAVED!!!\", path_checkpoint)\n",
    "            plt.plot(plot_x, plot_y1, 'bo', plot_x, plot_y2, 'go')\n",
    "            plt.show()\n",
    "        else:\n",
    "            saver.restore(session, path_checkpoint)\n",
    "            print(\"Model restored from file: %s\" % path_checkpoint)\n",
    "            if Predict == True:\n",
    "                print(\"Prediction for submit...\")\n",
    "                x_predict = Xs\n",
    "                y_predict = np.zeros((Xs.shape[0],Xs.shape[1]))\n",
    "\n",
    "                predict_generator = batch_generator(x_predict, y_predict, batch_size, sequence_length,\n",
    "                                                       online=True, online_shift=shift)\n",
    "\n",
    "                full_prediction = []\n",
    "\n",
    "                iteration = int(x_predict.shape[0] / shift)\n",
    "                #print(\"iteration: %i, predgen %s\" % (iteration, predict_generator))\n",
    "                print(\"#of validation points:\", x_predict.shape[0], \"#datapoints covers from minibatch:\",\n",
    "                      batch_size * sequence_length, \"iterations/epoch\", iteration)\n",
    "\n",
    "                for itr in range(iteration):\n",
    "                    x_validate_batch, y_validate_batch = next(predict_generator)\n",
    "                    #print (itr)\n",
    "                    __y_pred = session.run(output, feed_dict={X: x_validate_batch, Y: y_validate_batch, keep_prob: 1.0})\n",
    "                    \n",
    "                    for i in range(batch_size):\n",
    "                        full_prediction.append(__y_pred[i*sequence_length])\n",
    "                    #print(__y_pred.shape)\n",
    "                    \n",
    "                full_prediction = np.array(full_prediction)\n",
    "                full_prediction = full_prediction.ravel()\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_validate_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_prediction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shft = x_train.shape[0]+x_test.shape[0]+2*sequence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_submit = full_prediction*std_y+mean_y\n",
    "sub_file = pd.DataFrame()\n",
    "sub_file[\"timestamp\"] = submit_X[\"timestamp\"][shft:]\n",
    "sub_file[\"target\"] = y_submit\n",
    "sub_file.to_csv('submit2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LB 0.4705"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
